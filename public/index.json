[{"content":"","date":"15 September 2022","permalink":"/tags/aws/","section":"Tags","summary":"","title":"AWS"},{"content":"Now that I have this blog fully migrated over from Wordpress to Hugo pages I wanted to share with you the deployment pipeline that I created that takes care of publishing new posts.\nOverview\u003e Overview # The AWS Code Pipeline is comprised of four stages that automatically take source code, compile it from markdown pages to HTML, stores it, and refreshes the CDN cache.\nSource\u003e Source # The code Pipeline is triggered when a commit is pushed to the Github repository that stores the source code for this blog (listed below if you are interested) wgarbutt/stuffaboutcloudhugo Repository for the markdown pages that make the current version of the stuffaboutcloud blog HTML 0 0 I setup a GitHub (version 2) connection provider and followed the prompts to allow Code Pipeline to monitor a specified repository and branch for any changes.\nBuild\u003e Build # The next step will trigger a CodeBuild Docker Container to be created. This small container allows for Hugo to be downloaded, installed, and run against the downloaded source files.\nThe container is a prebuilt Amazon Linux 2 install and takes around a minute to stand up.\nOnce the container is fully initialized Codebuild will look for and run a user defined script called buildspec.yml\nversion: 0.2 phases: install: commands: - apt-get update - echo Installing hugo - curl -L -o hugo.deb https://github.com/gohugoio/hugo/releases/download/v0.102.3/hugo_0.102.3_Linux-64bit.deb - dpkg -i hugo.deb pre_build: commands: - echo In pre_build phase.. - echo Current directory is $CODEBUILD_SRC_DIR - ls -la build: commands: - hugo -v artifacts: files: - \u0026#39;**/*\u0026#39; base-directory: public This very simple script is split up into stages, Install, Pre-Build, Build and Artifacts\nInstall Phase:\nUpdates the package lists from the Linux update repository Downloads and installs Hugo Pre_build:\nCopy over source artifact (the source files downloaded from GitHub) Build:\nRuns the Hugo build command to convert Markdown source files into nicely deployable HTML pages Artifacts:\nMakes the converted HTML pages available to CodePipeline ready for the next stage Deploy\u003e Deploy # This stage takes the artifact from the build stage and writes the files to the S3 hosting bucket that runs this blog\nRefresh\u003e Refresh # The issue with having this blog hosting via a CloudFront distribution is that the cache takes some to refresh. I found a couple of posts on Reddit and other blogs suggesting that a simple Lambda function could be used to force an invalidation of the CloudFront cache.\n****def lambda_handler(event, context): job_id = event[\u0026#34;CodePipeline.job\u0026#34;][\u0026#34;id\u0026#34;] try: user_params = json.loads( event[\u0026#34;CodePipeline.job\u0026#34;] [\u0026#34;data\u0026#34;] [\u0026#34;actionConfiguration\u0026#34;] [\u0026#34;configuration\u0026#34;] [\u0026#34;UserParameters\u0026#34;] ) cloud_front.create_invalidation( DistributionId=user_params[\u0026#34;distributionId\u0026#34;], InvalidationBatch={ \u0026#34;Paths\u0026#34;: { \u0026#34;Quantity\u0026#34;: len(user_params[\u0026#34;objectPaths\u0026#34;]), \u0026#34;Items\u0026#34;: user_params[\u0026#34;objectPaths\u0026#34;], }, \u0026#34;CallerReference\u0026#34;: event[\u0026#34;CodePipeline.job\u0026#34;][\u0026#34;id\u0026#34;], }, ) except Exception as e: code_pipeline.put_job_failure_result( jobId=job_id, failureDetails={ \u0026#34;type\u0026#34;: \u0026#34;JobFailed\u0026#34;, \u0026#34;message\u0026#34;: str(e), }, ) else: code_pipeline.put_job_success_result( jobId=job_id, ) When the stage is triggered, it passes the pre-defined distrubutionID and objectPaths parameters to the Lambda function. The function uses Boto3 SDK for Python to call AWS CLI commands, in this case cloud_front.create_invalidation\nDeploy!\u003e Deploy! # And with that, all I have to do now is save and commit this .md file and push my changes to GitHub and within a few minutes this post will be live!\n","date":"15 September 2022","permalink":"/posts/hugo-pipeline/","section":"Posts","summary":"Now that I have this blog fully migrated over from Wordpress to Hugo pages I wanted to share with you the deployment pipeline that I created that takes care of publishing new posts.","title":"Hugo Publishing Pipeline"},{"content":"","date":"15 September 2022","permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":"15 September 2022","permalink":"/","section":"StuffAbout.Cloud","summary":"","title":"StuffAbout.Cloud"},{"content":"","date":"15 September 2022","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"Migrating Wordpress to Hugo\u003e Migrating Wordpress to Hugo # After my first full month hosting this blog on an EC2 instance (link here) I was billed roughly $20. Coincidently the same day my bill arrived I found a post on Reddit suggesting to someone to move away from Wordpress and to utilise a static webpage generated via Hugo\nWhat is Hugo\u003e What is Hugo # Hugo is a static site generator written in Go which produces incredibly fast and efficient HTML pages from Markdown files\nInstalling Hugo\u003e Installing Hugo # Installing Hugo was pretty straightforward. I already had Chocolatey installed so went the easy approach and simply ran\nchoco install hugo -confirm There are also options to install via scope or if you have go installed, compile via GitHub\nI then used the QuickStart guide to setup a example website so I could get an idea of what I needed\nhugo new site quickstart Next was time to choose a theme. I went over to themes.gohugo.io and had a look around, finally choosing Hello Friend (a little bit because it reminded me of Mr. Robot)\nInstallation of the theme was very straightforward, simply submodule the git location to your site folder and that’s it.\nMigrating Wordpress\u003e Migrating Wordpress # I needed to export my existing Wordpress posts to markdown files in order to import them into my new Hugo site There were a lot of options available, but I eventually ended up using this https://github.com/SchumacherFM/wordpress-to-hugo-exporter custom Wordpress plugin to convert the posts to markdown.\nOnce I had the files I needed to do a bit of cleanup such as resizing and relinking my images and cleaning up some leftover html code, it didn’t take me all that long to complete luckily.\nHugo allows you to deploy a local web server so you may launch your site from your local machine. Once I was happy with how it looked I was able to package up my site using the command Hugo -D\nThis created a folder named Public inside my site folder. The contents of this Public folder is what I needed to deploy to my S3 bucket that will become the host point for this blog\nI created an S3 bucket with the same name as this blogs url, turned on public access and static webpage hosting and set the default page to index.html\nNext we need to upload the contents from your Hugo/Public folder to your new S3 bucket. Once that’s done you’ll have a fully functional Hugo website running on AWS!\nDomain\u003e Domain # I wanted this site to work with my purchased domain name and to also work on HTTPS. I utilised AWS ACM console to create a certificate for my domain that I could use to secure my blog. The process for that is rather straightforward following this article from AWS https://docs.aws.amazon.com/acm/latest/userguide/gs-acm-request-public.html I chose the DNS validation option. This certificate is free to secure services you deploy on AWS.\nWith certificate sorted I moved onto creating a cloud front distribution. I pointed cloud front to the s3 bucket I uploaded my Hugo files to earlier and secured the domain with the certificate I’d just created.\nAnd that’s it! You know it’s worked because that’s how you are reading this blog. I’ll created a new post soon detailing my code pipeline work to deploy posts on this blog which I am very proud of.\n","date":"15 September 2022","permalink":"/posts/hugo-to-wordpress/","section":"Posts","summary":"Migrating Wordpress to Hugo\u003e Migrating Wordpress to Hugo # After my first full month hosting this blog on an EC2 instance (link here) I was billed roughly $20.","title":"WordPress to Hugo"},{"content":"WordPress on EC2\u003e WordPress on EC2 # I originally had this blog hosted on Azure with a borrowed MSDN subscription credit with the idea that I would eventually get around to document what I had learnt. Unfortunately the MSDN account I was using expired before I realised.\nSilver lining is I get to do it all over again, this time in AWS, and im documenting it right away so I dont fall for the same mistake again\nInfrastructure\u003e Infrastructure # The networking side of the infrastructure consists of a new VPC with a single AZ Subnet attached. With that I deployed a new Internet Gateway and new route table. I have a security group allowing SSH (Port22) and HTTPS (Port 443)\nI deployed a new EC2 instance and attached the networking above, configured Session Manager and attached an existing keypair.\nThis was deployed via CloudFormation (Github here)\nInstalling WordPress and SQL DB\u003e Installing WordPress and SQL DB # I am currently going through the AWS Certified Solutions Architect – Associate training course by Adrian Cantrill (Website here) and coincidently am up to a video where they do just this process. I have modified Adrian’s supplied script slightly.\n# DBName=database name for wordpress # DBUser=mariadb user for wordpress # DBPassword=password for the mariadb user for wordpress # DBRootPassword = root password for mariadb STEP 1 - Configure Authentication Variables which are used below\u003e STEP 1 - Configure Authentication Variables which are used below # DBName=\u0026#39;stuffaboutcloudwp\u0026#39; DBUser=\u0026#39;stuffaboutcloudwp\u0026#39; DBPassword=\u0026#39;REPLACEME\u0026#39; DBRootPassword=\u0026#39;REPLACEME\u0026#39; STEP 2 - Install system software - including Web and DB\u003e STEP 2 - Install system software - including Web and DB # sudo yum install -y mariadb-server httpd wget sudo amazon-linux-extras install -y lamp-mariadb10.2-php7.2 php7.2 STEP 3 - Web and DB Servers Online - and set to startup\u003e STEP 3 - Web and DB Servers Online - and set to startup # sudo systemctl enable httpd sudo systemctl enable mariadb sudo systemctl start httpd sudo systemctl start mariadb STEP 4 - Set Mariadb Root Password\u003e STEP 4 - Set Mariadb Root Password # mysqladmin -u root password $DBRootPassword STEP 5 - Install WordPress\u003e STEP 5 - Install WordPress # sudo wget http://wordpress.org/latest.tar.gz -P /var/www/html cd /var/www/html sudo tar -zxvf latest.tar.gz sudo cp -rvf wordpress/* . sudo rm -R wordpress sudo rm latest.tar.gz STEP 6 - Configure WordPress\u003e STEP 6 - Configure WordPress # sudo cp ./wp-config-sample.php ./wp-config.php sudo sed -i \u0026#34;s/\u0026#39;database_name_here\u0026#39;/\u0026#39;$DBName\u0026#39;/g\u0026#34; wp-config.php sudo sed -i \u0026#34;s/\u0026#39;username_here\u0026#39;/\u0026#39;$DBUser\u0026#39;/g\u0026#34; wp-config.php sudo sed -i \u0026#34;s/\u0026#39;password_here\u0026#39;/\u0026#39;$DBPassword\u0026#39;/g\u0026#34; wp-config.php sudo chown apache:apache * -R STEP 7 Create WordPress DB\u003e STEP 7 Create WordPress DB # echo \u0026#34;CREATE DATABASE $DBName;\u0026#34; \u0026amp;gt;\u0026amp;gt; /tmp/db.setup echo \u0026#34;CREATE USER \u0026#39;$DBUser\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;$DBPassword\u0026#39;;\u0026#34; \u0026amp;gt;\u0026amp;gt; /tmp/db.setup echo \u0026#34;GRANT ALL ON $DBName.* TO \u0026#39;$DBUser\u0026#39;@\u0026#39;localhost\u0026#39;;\u0026#34; \u0026amp;gt;\u0026amp;gt; /tmp/db.setup echo \u0026#34;FLUSH PRIVILEGES;\u0026#34; \u0026amp;gt;\u0026amp;gt; /tmp/db.setup mysql -u root --password=$DBRootPassword \u0026amp;lt; /tmp/db.setup sudo rm /tmp/db.setup STEP 8 - Browse to http://your_instance_public_ipv4_ip\u003e STEP 8 - Browse to http://your_instance_public_ipv4_ip # Configuring HTTPS\u003e Configuring HTTPS # First step was to point my domain at my EC2 instances public IP address, this is important as the tools we use below needs to verify I own my domain name.\nNext I followed this post made by AWS on how to configure Lets Encrypt on an EC2 instance, near the last section of the post you will see a heading named Certificate automation: Let’s Encrypt with Certbot on Amazon Linux 2\nThis will install all the components needed to use the free Lets Encrypt service and setup a Crontab to enable automatic cert renewal. Very cool\nUpdating PHP from 7.2 to 7.4\u003e Updating PHP from 7.2 to 7.4 # Some where in my deployment steps above I managed to install an older version of PHP. I was warned of this when I first logged on to WordPress, stating I was using an unsecure version of PHP and that I needed to update it as soon as possible.\nAnother blog to the rescue! I found this handy blog post that detailed the steps needed to upgrade PHP\nRestoring WordPress from backup\u003e Restoring WordPress from backup # Luckily before my Azure Subscription ran dry I had the wisdom to install a plugin named Updraft that has a free option to back up your entire WordPress deployment to cloud storage such as AWS S3. I simply had to redownload the plugin on my new deployment, point it towards my existing S3 bucket and restore! Cost\u003e Cost # Its early days so far but I estimate this will cost me around $10 a month. This is running on a very small t2.micro EC2 which seems to be running things a lot quicker than my Marketplace deployment of WordPress in Azure, which coincidently drew down my monthly credit by $60 a month.\n","date":"11 August 2022","permalink":"/posts/wordpress-on-ec2/","section":"Posts","summary":"WordPress on EC2\u003e WordPress on EC2 # I originally had this blog hosted on Azure with a borrowed MSDN subscription credit with the idea that I would eventually get around to document what I had learnt.","title":"WordPress on EC2"},{"content":"","date":"1 August 2022","permalink":"/tags/docker/","section":"Tags","summary":"","title":"docker"},{"content":"I thought I would share my first attempt at a docker compose file. This file deploys the containers I need to download and manage my media files.\nThe first part of this script deploys NZBGet, Sonarr, Radarr, and Plex containers and links to volumes for configuration data and NAS storage will be found.\nversion: \u0026#34;3.4\u0026#34; services: nzbget: container_name: nzbget image: linuxserver/nzbget:latest restart: unless-stopped network_mode: host environment: - TZ=${TZ} # timezone, defined in .env - PUID=0 - PGID=0 volumes: - downloads:/downloads # download folder - nzbgetconfig:/config # config files sonarr: container_name: sonarr image: linuxserver/sonarr:latest restart: unless-stopped network_mode: host environment: - PUID=0 - PGID=0 - TZ=${TZ} # timezone, defined in .env volumes: - sonarrconfig:/config # config files - tv:/TV # tv shows folder - downloads:/downloads # download folder radarr: container_name: radarr image: linuxserver/radarr:latest restart: unless-stopped network_mode: host environment: - PUID=0 - PGID=0 - TZ=${TZ} # timezone, defined in .env volumes: - radarrconfig:/config # config files - movies:/Movies # movies folder - downloads:/downloads # download folder plex-server: container_name: plex-server image: plexinc/pms-docker:latest restart: unless-stopped environment: - PUID=0 - PGID=0 - TZ=${TZ} # timezone, defined in .env network_mode: host volumes: - plexconfig:/config # plex database - plexconfig:/transcode # temp transcoded files - tv:/tv # media library - movies:/movies # media library The second part setups up the actual volumes, I have mounted my NAS as a CIFS share on my Docker VM so media can be shared between Docker and other VMs when needed\nvolumes: downloads: driver: local driver_opts: type: \u0026#39;none\u0026#39; o: \u0026#39;bind\u0026#39; device: \u0026#39;/data/container_config/nas/Downloads\u0026#39; tv: driver: local driver_opts: type: \u0026#39;none\u0026#39; o: \u0026#39;bind\u0026#39; device: \u0026#39;/data/container_config/nas/TV\u0026#39; movies: driver: local driver_opts: type: \u0026#39;none\u0026#39; o: \u0026#39;bind\u0026#39; device: \u0026#39;/data/container_config/nas/Movies\u0026#39; plexconfig: driver: local driver_opts: type: \u0026#39;none\u0026#39; o: \u0026#39;bind\u0026#39; device: \u0026#39;/data/container_config/plex\u0026#39; sonarrconfig: driver: local driver_opts: type: \u0026#39;none\u0026#39; o: \u0026#39;bind\u0026#39; device: \u0026#39;/data/container_config/sonarr\u0026#39; radarrconfig: driver: local driver_opts: type: \u0026#39;none\u0026#39; o: \u0026#39;bind\u0026#39; device: \u0026#39;/data/container_config/radarr\u0026#39; nzbgetconfig: driver: local driver_opts: type: \u0026#39;none\u0026#39; o: \u0026#39;bind\u0026#39; device: \u0026#39;/data/container_config/nzbget\u0026#39; I actually deploy and manage this stack via Portainer as I find at this point in time the GUI very useful. Over time I hope to remove the reliance of the GUI in favour of command line.\n","date":"1 August 2022","permalink":"/posts/2022-08-01-docker-compose-media-stack/","section":"Posts","summary":"I thought I would share my first attempt at a docker compose file.","title":"Docker Compose – Media Stack"},{"content":"","date":"16 July 2022","permalink":"/series/cloud-resume-challenge/","section":"Series","summary":"","title":"Cloud Resume Challenge"},{"content":"I’ve tackled some steps of the challenge out of turn here for a bit as it made sense to me to work on the Database/Lambda/API before working on the Javascript that requires them.\nStep 8: Database\u003e Step 8: Database # For this step I really leaned into trying to deploy via CloudFormation first rather than deploying via the GUI.\nOur Database will consist of a DynamoDB as per the recommendation on the Cloud Resume Challenge website. Our table will consist of two attributes, record_id and record_count.\nI ran into an issue not quite understanding how CloudFormation wanted to have the attributes presented, in the end I created just the key attribute of record_id and had to manually add the record_count attribute from the gui.\nI followed the Cloud Foundation documentation on DynamoDB here\nHere is my the relevant CloudFoundation YAML code:\nVisitorCountTable: Type: AWS::DynamoDB::Table Properties: AttributeDefinitions: - AttributeName: \u0026#34;record_id\u0026#34; AttributeType: \u0026#34;S\u0026#34; TableName: \u0026#34;Visitor_Count\u0026#34; ProvisionedThroughput: ReadCapacityUnits: 5 WriteCapacityUnits: 5 KeySchema: - AttributeName: \u0026#34;record_id\u0026#34; KeyType: \u0026#34;HASH\u0026#34; Once created, log into the AWS GUI and find your DynamoDB table. Click add item, add the attribute record_count with an value of 1\nStep 8.5 and 10: Lambda and Python\u003e Step 8.5 and 10: Lambda and Python # We’ll need to create two Lambda functions, one to get the visitor count and one to increase it.\nI again went with the CloudFormation first approach and found I was starting to get the hang of it.\nI followed the documentation on Lambda functions here\nFirst I manually created a IAM role for my Lambda functions, I included the following policies\nFor this to work, we will need to create a couple of small Python scripts that the function will invoke\ngetvisitor.py simply connects to our DynamoDB database and returns the current value of the record_count attribute\nimport json import boto3 dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(\u0026#39;Visitor_Count\u0026#39;) def lambda_handler(event, context): response = table.get_item(Key={ \u0026#39;record_id\u0026#39;:\u0026#39;0\u0026#39; }) return response\u0026amp;#91;\u0026#39;Item\u0026#39;]\u0026amp;#91;\u0026#39;record_count\u0026#39;] addvisitor.py is similar, this time increasing the current value of the record_count attribute.\nimport json import boto3 dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(\u0026#39;Visitor_Count\u0026#39;) def lambda_handler(event, context): response = table.get_item(Key={ \u0026#39;record_id\u0026#39;:\u0026#39;0\u0026#39; }) record_count = response\u0026amp;#91;\u0026#39;Item\u0026#39;]\u0026amp;#91;\u0026#39;record_count\u0026#39;] record_count = record_count + 1 print(record_count) response = table.put_item(Item={ \u0026#39;record_id\u0026#39;:\u0026#39;0\u0026#39;, \u0026#39;record_count\u0026#39;: record_count }) return \u0026#34;Records added successfully!\u0026#34; We need to save and zip these two files up and upload them to a S3 bucket so the Lambda function can access\nNow deploy it with the CloudFoundation YAML Code\nLambdaGetVisitor: Type: \u0026#34;AWS::Lambda::Function\u0026#34; Properties: FunctionName: \u0026#34;CloudResumeGetVisitor\u0026#34; Handler: \u0026#34;getvisitor.lambda_handler\u0026#34; Architectures: - \u0026#34;x86_64\u0026#34; Code: S3Bucket: \u0026#34;cloudresumelambdafunctions\u0026#34; S3Key: \u0026#34;getvisitor.zip\u0026#34; Role: \u0026#34;arn:aws:iam::337461354076:role/CloudResume-LambdaFunctions\u0026#34; Runtime: \u0026#34;python3.9\u0026#34; LambdaAddVisitor: Type: \u0026#34;AWS::Lambda::Function\u0026#34; Properties: FunctionName: \u0026#34;CloudResumeAddVisitor\u0026#34; Handler: \u0026#34;addvisitor.lambda_handler\u0026#34; Architectures: - \u0026#34;x86_64\u0026#34; Code: S3Bucket: \u0026#34;cloudresumelambdafunctions\u0026#34; S3Key: \u0026#34;addvisitor.zip\u0026#34; Role: \u0026#34;arn:aws:iam::337461354076:role/CloudResume-LambdaFunctions\u0026#34; Runtime: \u0026#34;python3.9\u0026#34; All going well, you should have two shiny functions in your gui\nStep 9: API\u003e Step 9: API # We’ll need to create ourselves a REST API with two methods being GET to reference our LambdaGetVisitor function and POST to reference our LambdaAddVisitor function.\nFirst we create the API Gateway object, as before I have preferred to go the CloudFormation root for this deployment\nApiGateway: Type: \u0026#34;AWS::ApiGateway::RestApi\u0026#34; Properties: Name: \u0026#34;visitors\u0026#34; ApiKeySourceType: \u0026#34;HEADER\u0026#34; EndpointConfiguration: Types: - \u0026#34;REGIONAL\u0026#34; Pretty straightforward that one.\nNext we move onto the GET method, I had a pretty rough time trying to figure this part out as pure CloudFormation code. I ended up completing this in the GUI, exported the code via Former2, deleting from GUI and redeploying with the newly written code.\nApiGatewayGet: Type: \u0026#34;AWS::ApiGateway::Method\u0026#34; Properties: RestApiId: !Ref ApiGateway ResourceId: !GetAtt ApiGateway.RootResourceId HttpMethod: \u0026#34;GET\u0026#34; AuthorizationType: \u0026#34;NONE\u0026#34; ApiKeyRequired: false RequestParameters: {} MethodResponses: - ResponseModels: \u0026#34;application/json\u0026#34;: \u0026#34;Empty\u0026#34; StatusCode: \u0026#34;200\u0026#34; Integration: CacheNamespace: !GetAtt ApiGateway.RootResourceId ContentHandling: \u0026#34;CONVERT_TO_TEXT\u0026#34; IntegrationHttpMethod: \u0026#34;POST\u0026#34; IntegrationResponses: - ResponseTemplates: {} StatusCode: \u0026#34;200\u0026#34; PassthroughBehavior: \u0026#34;WHEN_NO_MATCH\u0026#34; TimeoutInMillis: 29000 Type: \u0026#34;AWS\u0026#34; Uri: !Sub \u0026#34;arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/arn:aws:lambda:${AWS::Region}:${AWS::AccountId}:function:${LambdaGetVisitor}/invocations\u0026#34; Same as above for the POST method, deployed via gui, exported code, deleted and redeployed via CloudFormation\nApiGatewayPost: Type: \u0026#34;AWS::ApiGateway::Method\u0026#34; Properties: RestApiId: !Ref ApiGateway ResourceId: !GetAtt ApiGateway.RootResourceId HttpMethod: \u0026#34;POST\u0026#34; AuthorizationType: \u0026#34;NONE\u0026#34; ApiKeyRequired: false RequestParameters: {} MethodResponses: - ResponseModels: \u0026#34;application/json\u0026#34;: \u0026#34;Empty\u0026#34; StatusCode: \u0026#34;200\u0026#34; Integration: CacheNamespace: !GetAtt ApiGateway.RootResourceId ContentHandling: \u0026#34;CONVERT_TO_TEXT\u0026#34; IntegrationHttpMethod: \u0026#34;POST\u0026#34; IntegrationResponses: - ResponseTemplates: {} StatusCode: \u0026#34;200\u0026#34; PassthroughBehavior: \u0026#34;WHEN_NO_MATCH\u0026#34; TimeoutInMillis: 29000 Type: \u0026#34;AWS\u0026#34; Uri: !Sub \u0026#34;arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/arn:aws:lambda:${AWS::Region}:${AWS::AccountId}:function:${LambdaAddVisitor}/invocations\u0026#34; Once I deployed I went to the GUI to test my methods, but found both reported a permission denied error when I tried to run them.\nWhen deploying from the GUI you get a prompt asking if you want to give your API method access to the Lambda function, obviously that didnt happen when deploying via CloudFormation. I found this help page that exactly explained my issue.\nHere are my code snippets to address that problem\nAPIGetPermissions: Type: AWS::Lambda::Permission Properties: Action: \u0026#34;lambda:InvokeFunction\u0026#34; FunctionName: !Ref LambdaGetVisitor Principal: \u0026#34;apigateway.amazonaws.com\u0026#34; SourceArn: arn:aws:execute-api:ap-southeast-2:337461354076:\u0026amp;lt;api-id\u0026amp;gt;/*/GET/ APIPostPermissions: Type: AWS::Lambda::Permission Properties: Action: \u0026#34;lambda:InvokeFunction\u0026#34; FunctionName: !Ref LambdaAddVisitor Principal: \u0026#34;apigateway.amazonaws.com\u0026#34; SourceArn: arn:aws:execute-api:ap-southeast-2:337461354076:\u0026amp;lt;api-id\u0026amp;gt;/*/POST/ Thats it for now, part 3 will try and merge this all together and get a nice counter on our Resume\n","date":"16 July 2022","permalink":"/posts/2022-07-16-cloud-resume-challenge-part-2/","section":"Posts","summary":"I’ve tackled some steps of the challenge out of turn here for a bit as it made sense to me to work on the Database/Lambda/API before working on the Javascript that requires them.","title":"Cloud Resume Challenge – Part 2"},{"content":"Step 7: Javascript\u003e Step 7: Javascript # We need to include Javascript to call upon our APIs we created earlier. This took me quite a bit of time as I was struggling to find a solution that worked for me, whether I did something wrong in the earlier parts or wasn’t quite understanding things correctly I’m not sure.\nBelow is my two Javascripts I created, the first one invokes out POST method, which increases the visitor count value on our DynamoDB\n\u0026lt;script\u0026gt; fetch(\u0026#39;https://5cabfxax81.execute-api.ap-southeast-2.amazonaws.com/Prod/\u0026#39;, { method: \u0026#39;POST\u0026#39;, headers: { \u0026#34;Content-type\u0026#34;: \u0026#34;application/x-www-form-urlencoded; charset=UTF-8\u0026#34; }, }) .then(response =\u0026gt; response.json()) .then(json =\u0026gt; { console.log(json); }); \u0026lt;/script\u0026gt; And the second to retrieve the value of the visitor count\n\u0026lt;script\u0026gt; fetch(\u0026#39;https://5cabfxax81.execute-api.ap-southeast-2.amazonaws.com/Prod/\u0026#39;) .then((response =\u0026gt; response.json())) .then((data) =\u0026gt; {document.getElementById(\u0026#34;counter\u0026#34;).innerHTML = data}) \u0026lt;/script\u0026gt; We then exchange the value counter for the number from the database\n\u0026lt;p class=\u0026#34;subDetails\u0026#34;\u0026gt;You are visitor number \u0026lt;span id=\u0026#34;counter\u0026#34;\u0026gt;loading...\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; Step 12: Infrastructure as Code\u003e Step 12: Infrastructure as Code # Throughout these steps I have made reference to using CloudFoundation as a first stop as much as possible. If I had to use the GUI first I would export the resources as code via Former2.\nExporting via Former2\nCreate a Read-Only IAM user with programmatic access keys, Former2 will use this to scan your infrastructure for deployed resources Run through the setup wizard, it will ask you to install a plugin for your browser, ask for credentials and any other parameters you want to specify. When finished press the Scan Account button. Once scan has completed, click the Dashboard button, you will be presented with all of AWS’s services. If you click on the service you need the code for you should see your resources listed with some check boxes. If you check the box the resource code will be added to the list to be generated Click generate, you will be presented with a screen with all of the resources you selected coded in YAML. My template.yaml is stored on my GitHub here. I’ve destroyed and reapplied my stack twice to confirm I haven’t missed anything. Very happy with it.\nStep 13/14/15: Source Control and CI/CD\u003e Step 13/14/15: Source Control and CI/CD # I’ve setup all of my code to store in GitHub. I save my edits in Visual Studio Code and commit to my GitHub repository.\nI have created a AWS Pipeline that monitors my repository for changes and when a commit is received, downloads the repository to the S3 bucket hosting my website and replaces the files, keeping the site up to date within a few seconds of pressing commit.\nI followed the post from Casey McMullen here\nStep 16: This Blog\u003e Step 16: This Blog # I’m not the greatest at communicating, both verbal and written, so writing this blog wasn’t something that came naturally to me. I hope this helps someone out there like other posts on the internet helped me. I am very open to feedback, so if you have anything to share please let me know. I can be reached at wgarbutt@gmail.com\n","date":"16 July 2022","permalink":"/posts/2022-07-16-cloud-resume-challenge-part-3/","section":"Posts","summary":"Step 7: Javascript\u003e Step 7: Javascript # We need to include Javascript to call upon our APIs we created earlier.","title":"Cloud Resume Challenge – Part 3"},{"content":"","date":"16 July 2022","permalink":"/tags/cloudresume/","section":"Tags","summary":"","title":"cloudresume"},{"content":"","date":"16 July 2022","permalink":"/series/","section":"Series","summary":"","title":"Series"},{"content":"Let me tell you about a neat little tool I recently discovered to make AWS logins easier. Its called AWS Vault, its developed by a third party named 99Designs. The link to the GitHub repository is here.\nAWS Vault allows you to store IAM credentials to your OS keystore and then generates temporary credentials from those to use for your chosen shell. I myself use Windows and Powershell.\nHere is a run through on the install process I followed. Make sure you already have AWS CLI installed as the profile and configurations located in ~/.aws/config are used as part of AWS Vaults configuration.\nInstall Chocolatey\u003e Install Chocolatey # There are a number of different ways to install AWS Vault, with one of them using Chocolatey. I have seen many references to Chocolatey that I decided this approach would be the most useful for myself.\nIts a simple one liner to get installed in Powershell:\nSet-ExecutionPolicy Bypass -Scope Process -Force [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString(\u0026#39;https://community.chocolatey.org/install.ps1\u0026#39;)) Install AWS Vault\u003e Install AWS Vault # Next we need to simply run the following command in Powershell to install AWS Vault\nchoco install aws-vault Configuring AWS Vault\u003e Configuring AWS Vault # Now run the command to add IAM credentials to the credential store. This will prompt you for the Access and Secret key for your IAM user, this only shows once on screen from the AWS console so you may need to create new a new keyset\naws-vault add \u0026amp;lt;myiamuser\u0026amp;gt; Enter Access Key ID: XXXXXXXXXX Enter Secret Key: XXXXXXXXXXXXX Now you should be all set to utilise the convenience of store credentials\nFor me, the biggest feature is the ability to login to the AWS console as any of your saved users\nFor example, I run aws-vault login will from powershell which logs me in to the AWS console for a short period of time, enough for me to complete anything I may need.\nI have recently started using AWS SAM to wrap around some CloudFormation code I have been playing with, all I need to do to deploy my updates is type aws-vault exec will --no-session -- sam deploy and the job is done, no credentials needed!\nHow it works\u003e How it works # AWS Vault uses Amazon\u0026rsquo;s STS service to generate temporary credentials via the GetSessionToken or AssumeRole API calls. These expire in a short period of time, so the risk of leaking credentials is reduced.\n","date":"14 July 2022","permalink":"/posts/2022-07-14-aws-vault/","section":"Posts","summary":"Let me tell you about a neat little tool I recently discovered to make AWS logins easier.","title":"AWS Vault"},{"content":"I\u0026rsquo;m the kind of person who is never done with learning and trying to align my skillset with industry demand. I\u0026rsquo;ve done a lot of VMWare training and certification over the years and decided it was time to branch out to one of the popular public cloud offerings.\nWhile researching Amazon Web Services courses and certifications I stumbled across the Cloud Resume Challenge. The challenge is a 16 step project designed to show off the skills typically required of a cloud engineer, specific to the flavour of cloud provider you are interested in (AWS, Azure, or Google).\nHere are the steps I took as part of the challenge, they are referenced from the Cloud Resume Challenge website\nhttps://cloudresumechallenge.dev/docs/the-challenge/aws/\nStep 1: Certification\u003e Step 1: Certification # For this I had to sit and pass the AWS Cloud Practitioner certification. (More details on this certification here)\nI used Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner – 2022 Udemy course ( Link) as well as his 6 practice exams ( Link).\nI found Stephanes course to be tightly packed and didn\u0026rsquo;t drag on too much, if you didn\u0026rsquo;t quite understand a particular topic you could always dive into it more using AWS\u0026rsquo;s available resources.\nThe practice exams gave very detailed explanations of the available answers and highlighted areas where I needed to study more.\nI sat the exam from home and was told to check in half an hour before my exam time. I really needed that extra half an hour as the identity and room setup verification took nearly that whole time.\nI passed the exam with 803/1000\nStep 2: HTML Resume\u003e Step 2: HTML Resume # When I first left school I signed up to do a Network engineering course. It had a very small module on web design and coding, that was the last time I ever wrote in HTML. A poster on Reddit suggested finding some inspiration on a site called Codepen.io. There I found a clean simple template that I could use as a starting point.\nI saved my code to my GitHub here. (Spoiler alert, you can see the finished project here)\nStep 3: CSS\u003e Step 3: CSS # My resume needed to be styled with CSS. I would be the worst choice for a game of Pictionary and my design imagination is non-existent. I scoped out Codepen.io again and luckily found a suitable template for my css file. I made a number of adjustments and learnt quite a bit about css while I was at it.\nI link to this file on my GitHub here\nStep 4: Static Website\u003e Step 4: Static Website # My HTML files are stored on an AWS S3 bucket configured to be a static website. I found this to be pretty straight forward to setup. Here is the steps I needed to do to configure a AWS S3 static website.\nDeploy your empty bucket. You\u0026rsquo;ll need to name the bucket the same name as your domain. AWS uses the host header in the web request to route traffic to the appropriate bucket Configure the bucket for static web hosting. On the properties of your bucket you should see an option “Static website hosting”, Select the option “Use this bucket to host a website”, enter “index.html” as the index document\nYour bucket is now configured for static website hosting and you should have an S3 website URL listed at the bottom of the bucket property page\nStep 5/6: DNS and HTTPS\u003e Step 5/6: DNS and HTTPS # I purchased myself a domain name will-garbutt.me for around $50. The challenge suggests using AWS Route 53 to host the DNS for your domain but since I am also doing some Terraform learning and playing with deploying resources using code I found that AWS considers a domain that was stood up, deleted, and stood up again to be two domains and charging me $10 each time I try.\nFor this I used Cloudflare as it has a well documented Terraform provider module and is free no matter how many times I have deployed and destroyed resources. In my Terrafrom GitHub Repository ( link) I have both AWS and Cloudflare code to setup DNS.\nCloudflare also has a simple way to enable HTTPS with just a click of a button.\nPart 2 will be coming soon!\n","date":"14 July 2022","permalink":"/posts/2022-07-14-cloud-resume-challenge-part-1/","section":"Posts","summary":"I\u0026rsquo;m the kind of person who is never done with learning and trying to align my skillset with industry demand.","title":"Cloud Resume Challenge – Part 1"},{"content":"A few months ago I was tasked with finding a solution for resetting root passwords for 800+ VMWare ESXi hosts on a regular schedule.\nWe had recently completed a project to move our credential store to Passwordstate and had noticed it had some functionality around automating resetting and validating credentials.\nI initially started looking at the built-in Linux scripts which utilises SSH connections, something we have disabled for our ESXi hosts for security.\nSearching through forums I found a post where someone used PowerCLI to do the heavy lifting but found the post didnt quite give me everything I needed to complete the project.\nI made some custom scripts that do a pretty good job at completing the solution.\nPassword reset and password validation scripts:\nWe need to talk about these custom scripts first, as we need the IDs of the script to fill in the JSON data for scripted host ingest\nSetting the ESXi Password Script:\u003e Setting the ESXi Password Script: # Function Set-ESXiPassword { [CmdletBinding()] param ( [String]$HostName, [String]$UserName, [String]$OldPassword, [String]$NewPassword ) try { $Connection = Connect-VIServer $HostName -User $UserName -Password $OldPassword } catch { switch -wildcard ($error[0].Exception.ToString().ToLower()) { \u0026#34;*incorrect user*\u0026#34; { Write-Output \u0026#34;Incorrect username or password on host \u0026#39;$HostName\u0026#39;\u0026#34; break } \u0026#34;*\u0026#34; { write-output $error[0].Exception.ToString().ToLower() break } } } try { $change = Set-VMHostAccount -UserAccount $UserName -Password $NewPassword | out-string if ($change -like \u0026#39;*root*\u0026#39;) { Write-Output \u0026#34;Success\u0026#34; } else { Write-Output \u0026#34;Failed\u0026#34; } Disconnect-Viserver * -confirm:$false } catch { switch -wildcard ($error[0].Exception.ToString().ToLower()) { \u0026#34;*not currently connected*\u0026#34; { Write-Output \u0026#34;It wasn\u0026#39;t possible to connect to \u0026#39;$HostName\u0026#39;\u0026#34; break } \u0026#34;*weak password*\u0026#34; { Write-Output \u0026#34;Failed to execute script correctly against Host \u0026#39;$HostName\u0026#39; for the account \u0026#39;$UserName\u0026#39;. It appears the new password did not meet the password complexity requirements on the host.\u0026#34; break } \u0026#34;*\u0026#34; { write-output $error[0].Exception.ToString().ToLower() break } default { Write-Output \u0026#34;Got here\u0026#34; } } } } Set-ESXiPassword -HostName \u0026#39;[HostName]\u0026#39; -UserName \u0026#39;[UserName]\u0026#39; -OldPassword \u0026#39;[OldPassword]\u0026#39; -NewPassword \u0026#39;[NewPassword]\u0026#39; This script logs into a host directly via powershell and powercli modules, Passwordstate passes the common variables such as OldPassword, NewPasswork, Username etc. The Set-VMHostAccount command is actually doing all the heavy lifting here. The rest of the script is some simple error handling.\nPowerCLI is baked into every ESXi host and only requires powershell to be open from the Passwordstate webserver to the host (port 443).\nPassword Validation Script\u003e Password Validation Script # Function Validate-ESXiPassword { [CmdletBinding()] param ( [String]$HostName, [String]$UserName, [String]$CurrentPassword ) $ErrorActionPreference = \u0026#34;Stop\u0026#34; try { $Connection = Connect-VIServer $HostName -User $UserName -Password $CurrentPassword if ($Connection.isconnected) { Write-Output \u0026#34;Success\u0026#34; } else { Write-Output \u0026#34;Failed\u0026#34; } } catch { switch -wildcard ($error[0].Exception.ToString().ToLower()) { \u0026#34;*incorrect user*\u0026#34; { Write-Output \u0026#34;Incorrect username or password on host \u0026#39;$HostName\u0026#39;\u0026#34; break Disconnect-VIServer $HostName -Force -Confirm:$false } default { Write-Output \u0026#34;Error is: $($error[0].Exception.message)\u0026#34; } } } } Validate-ESXiPassword -HostName \u0026#39;[HostName]\u0026#39; -UserName \u0026#39;[UserName]\u0026#39; -CurrentPassword \u0026#39;[CurrentPassword]\u0026#39; This script simply attempts to connect to the host in question via Powershell.\nThe success criteria simply looks for the word root in the output, this may be foolish of me, but there isn\u0026rsquo;t much of a result from the command to parse for a successful result\nIf the command fails it should be captured by my catch commands\nHost/Password Entry:\u003e Host/Password Entry: # All of our hosts are domain joined so host discovery was rather straightforward enough by using the built in utility in Passwordstate. Unfortunately there was no easy way to automatically discover host accounts, but since we are only dealing with Root here we can script adding of password entries. You\u0026rsquo;ll need to get your custom script IDs from the ones you created above. This is a one off script and took around one minute to add 800 hosts\nConnect-VIServer (your vcenter server) $hostlist = get-vmhost $Creds = Get-Credential $PasswordstateUrl = \u0026#39;https://passwordstateurl/winapi/passwords\u0026#39; foreach ($hostname in $hostlist) { Write-Host \u0026#34;I am working on host $($Hostname.name)\u0026#34; $jsonData = \u0026#39; { \u0026#34;PasswordListID\u0026#34;:\u0026#34;existingpasswordlistID\u0026#34;, \u0026#34;Title\u0026#34;:\u0026#34;\u0026#39; + $($hostname.name) + \u0026#39;\u0026#34;, \u0026#34;UserName\u0026#34;:\u0026#34;root\u0026#34;, \u0026#34;password\u0026#34;:\u0026#34;existingpassword\u0026#34;, \u0026#34;hostname\u0026#34;:\u0026#34;\u0026#39; + $($hostname.name) + \u0026#39;\u0026#34;, \u0026#34;AccountTypeID\u0026#34;: \u0026#34;34\u0026#34;, (VMWare) \u0026#34;PasswordResetEnabled\u0026#34;: false, \u0026#34;EnablePasswordResetSchedule\u0026#34;: true, \u0026#34;ScriptID\u0026#34;: \u0026#34;28\u0026#34;, \u0026#34;HeartbeatEnabled\u0026#34;: true, \u0026#34;ValidationScriptID\u0026#34;: \u0026#34;22\u0026#34;, } \u0026#39; Write-Host $jsondata $result = Invoke-Restmethod -Method Post -Uri $PasswordstateUrl -ContentType \u0026#34;application/json\u0026#34; -Body $jsonData -Credential $Creds } Write-Host \u0026#34;Disconnecting vCenter\u0026#34; Disconnect-Viserver * -confirm:$false ","date":"5 July 2022","permalink":"/posts/2022-07-05-automated-password-resetting-for-esxi-hosts/","section":"Posts","summary":"A few months ago I was tasked with finding a solution for resetting root passwords for 800+ VMWare ESXi hosts on a regular schedule.","title":"Automated password resetting for ESXi Hosts"},{"content":"","date":"5 July 2022","permalink":"/tags/vmware/","section":"Tags","summary":"","title":"VMWare"},{"content":"","date":"1 January 0001","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"1 January 0001","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"}]